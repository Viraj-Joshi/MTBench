params:
  seed: ${...seed}
  algo:
    name: mt_sac

  model:
    name: mt_continuous_sac

  network:
    name: soft_actor_critic
    separate: True
    space:
      continuous:
    mlp:
      units: [512, 256, 128]
      activation: relu
      
      initializer:
        name: default
    log_std_bounds: [-20, 2]

  load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
  load_path: ${...checkpoint} # path to the checkpoint to load

  config:
    name: ${resolve_default:mt_sac,${....experiment}} 
    full_experiment_name: ${.name}
    env_name: rlgpu
    multi_gpu: ${....multi_gpu}
    mixed_precision: False
    normalize_input: True
    value_bootstrap: True
    normalize_value: True
    num_actors: ${....task.env.numEnvs}
    reward_shaper:
      scale_value: 1.0
    gamma: 0.99
    gradient_steps_per_itr: ${....gradient_steps_per_itr} # Number of optimization steps where new batch is sampled and critic + actor are updated

    init_alpha: 1
    alpha_lr: 5e-3
    actor_lr: 5e-4
    critic_lr: 5e-4
    critic_tau: .01
    batch_size: 8192
    nstep: ${....nstep}         # the number of steps to use for the multi-step return
    actor_update_freq: 1
    critic_target_update_freq: 1

    max_epochs: ${resolve_default:20000,${....max_iterations}}
    horizon: 1 # samples new batch and performs a gradient step times gradient_steps_per_itr*horizon times per epoch
    save_best_after: 200
    save_frequency: 60000
    print_stats: True
    grad_norm: .5
    learnable_temperature: True
    num_warmup_steps: ${....nstep}  # horizon length to do random actions before starting to learn 
    replay_buffer_size: 5000000

    target_entropy_coef: 1.0
    use_disentangled_alpha: True # learn a different entropy coefficient for each task
    use_replay_ratio_scaling: False