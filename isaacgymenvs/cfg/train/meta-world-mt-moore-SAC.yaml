params:
  seed: ${...seed}
  algo:
    name: mt_sac

  model:
    name: mt_continuous_sac

  network:
    name: moore_sac
    separate: True # False not supported, actor and critic cannot be shared

    space:
      continuous:
    log_std_bounds: [-10, 2]
    moore:
      num_experts: 4
      num_layers: 3
      D: 400
      activation: relu
      agg_activation: [Linear, Tanh]
      initializer:
        name: kaiming_normal
    head:
      units: [256,256]
      activation: relu
      d2rl: False
      initializer:
        name: kaiming_normal
    task_encoder:
      units: [256]
      activation: relu
      d2rl: False
      initializer:
        name: kaiming_normal

  load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
  load_path: ${...checkpoint} # path to the checkpoint to load

  config:
    name: ${resolve_default:sac_mt_${....task_id},${....experiment}} 
    full_experiment_name: ${.name}
    env_name: rlgpu
    multi_gpu: ${....multi_gpu}
    mixed_precision: False
    normalize_input: True
    value_bootstrap: True
    normalize_value: False
    num_actors: ${....task.env.numEnvs}
    init_at_random_progress: True
    reward_shaper:
      scale_value: 1.0
    gamma: 0.99
    tau: 0.95
    gradient_steps_per_itr: 2 # gradient_steps_per_itr i.e Number of optimization steps that should occur before the training step is over and a new batch of transitions is collected by the actor.

    init_alpha: 1
    alpha_lr: 1e-4
    actor_lr: 3e-4
    critic_lr: 3e-4
    critic_tau: 5e-3
    batch_size: 4096

    max_epochs: ${resolve_default:20000,${....max_iterations}}
    num_steps_per_episode: 1
    save_best_after: 200
    save_frequency: 1000
    print_stats: True
    grad_norm: 1.0
    learnable_temperature: True
    num_warmup_steps: 3 # total number of warmup steps: num_actors * num_steps_per_episode * num_warmup_steps
    replay_buffer_size: 4_000_000
    target_entropy_coef: 1.0
    use_disentangled_alpha: True